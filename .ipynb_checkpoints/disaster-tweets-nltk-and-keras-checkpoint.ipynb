{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Summary\n",
    "The goal is to classify disaster tweets. With this notebook I managed to get 0.79 score. This notebook consists of 3 parts:\n",
    "1. Text Preprocessing\n",
    "2. Vectorizing (with CountVectorizer)\n",
    "3. Keras Model Check\n",
    "4. Predictions\n",
    "\n",
    "I am very new to NLP and deep learning so this notebook is based on my limited knowledge. See also these notebooks for reference:\n",
    "1. https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove\n",
    "2. https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   keyword   7552 non-null   object\n",
      " 1   location  5080 non-null   object\n",
      " 2   text      7613 non-null   object\n",
      " 3   target    7613 non-null   int64 \n",
      " 4   set       7613 non-null   object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 297.5+ KB\n",
      "None\n",
      "  keyword location                                               text  target  \\\n",
      "0     NaN      NaN  Our Deeds are the Reason of this #earthquake M...       1   \n",
      "1     NaN      NaN             Forest fire near La Ronge Sask. Canada       1   \n",
      "2     NaN      NaN  All residents asked to 'shelter in place' are ...       1   \n",
      "3     NaN      NaN  13,000 people receive #wildfires evacuation or...       1   \n",
      "4     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...       1   \n",
      "\n",
      "     set  \n",
      "0  train  \n",
      "1  train  \n",
      "2  train  \n",
      "3  train  \n",
      "4  train  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv', encoding='utf-8')\n",
    "df_new = pd.read_csv('test.csv', encoding='utf-8')\n",
    "df['set'] = 'train'\n",
    "df_new['set'] = 'pred'\n",
    "df = df.drop(columns='id')\n",
    "df_new = df_new.drop(columns='id')\n",
    "data = pd.concat([df, df_new]) # concatenate for text preprocessing\n",
    "data.reset_index(inplace=True)\n",
    "print(df.info())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check target class balance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.57034\n",
      "1    0.42966\n",
      "Name: target, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(df.target.value_counts(normalize=True))\n",
    "df.target.value_counts(normalize=True).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "Steps:\n",
    "0. Convert text to lowercase\n",
    "1. Remove unwanted characters\n",
    "2. Contractions expander\n",
    "3. Word tokenizer (including stop words & punctuation removal).\n",
    "4. Hashtag finder & counter\n",
    "5. Mention finder & counter\n",
    "6. Url finder & counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to lowercase\n",
    "data.text = data.text.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove/replace unwanted characters\n",
    "data.text = data.text.str.replace('ûª', \"'\") # replace with '\n",
    "data.text = data.text.str.replace('', \" \")\n",
    "data.text = data.text.str.replace('û', \" \")\n",
    "data.text = data.text.str.replace('ò', \" \")\n",
    "data.text = data.text.str.replace('ó', \" \")\n",
    "data.text = data.text.str.replace('ï', \" \")\n",
    "data.text = data.text.str.replace('ì', \" \")\n",
    "data.text = data.text.str.replace('÷', \" \")\n",
    "data.text = data.text.str.replace('åê', \" \")\n",
    "data.text = data.text.str.replace('##', \"#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contractions Expander\n",
    "import re\n",
    "contractions_dict = { \n",
    "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is / how does\",\n",
    "\"i'd\": \"i had / i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i shall / i will\",\n",
    "\"i'll've\": \"i shall have / i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "}\n",
    "c_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "def expandContractions(text, c_re=c_re):\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "    return c_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-2626473beecb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcasual\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTweetTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "\n",
    "# Tweet-tokenizer function\n",
    "def tweet_func(dataframe):\n",
    "    \"\"\"Tokenize tweet, stop words removal, and punctuation removal from 'text' column of dataframe\"\"\"\n",
    "    tweets = dataframe.text # tweets are in df.text\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tweet_tokenizer = TweetTokenizer() # nltk.tokenize.word_tokenize also works!!\n",
    "    tweet_clean = []\n",
    "    n_token = []\n",
    "    for text in tweets:\n",
    "        text = re.sub('#\\w+', '', text) # Remove hashtags\n",
    "        text = re.sub('@\\w+', '', text) # Remove mentions\n",
    "        text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text) # Remove url\n",
    "        token = tweet_tokenizer.tokenize(text) # Tweet Tokenizing\n",
    "        token = [word for word in token if not word in stop_words] # Stop Words Removal\n",
    "        token = [''.join(s) for s in token if not s in string.punctuation] # Punctuation Removal\n",
    "        tweet_clean.append(' '.join(token))\n",
    "        n_token.append(len(token)) # Word (Token) Count\n",
    "    return (tweet_clean, n_token)\n",
    "\n",
    "# Hashtag finder & counter\n",
    "def hashtag_func(dataframe):\n",
    "    \"\"\"Hashtag finder & counter from 'text' column of dataframe\"\"\"\n",
    "    tweets = dataframe.text # tweets are in df.text\n",
    "    hashtag_list = []\n",
    "    hashtag_count = []\n",
    "    for text in tweets:\n",
    "        hashtag = re.findall('#\\w+', text)\n",
    "        hashtag_list.append(' '.join(hashtag))\n",
    "        hashtag_count.append(len(hashtag))\n",
    "    return (hashtag_list, hashtag_count)\n",
    "\n",
    "# Mention finder & counter\n",
    "def mention_func(dataframe):\n",
    "    \"\"\"Mention finder & counter from 'text' column of dataframe\"\"\"\n",
    "    tweets = dataframe.text # tweets are in df.text\n",
    "    mention_list = []\n",
    "    mention_count = []\n",
    "    for text in tweets:\n",
    "        mention = re.findall('@\\w+', text)\n",
    "        mention_list.append(' '.join(mention))\n",
    "        mention_count.append(len(mention))\n",
    "    return (mention_list, mention_count)\n",
    "\n",
    "# Url finder & counter\n",
    "def url_func(dataframe):\n",
    "    \"\"\"URL finder & counter from 'text' column of dataframe\"\"\"\n",
    "    tweets = dataframe.text # tweets are in df.text\n",
    "    url_list = []\n",
    "    url_count = []\n",
    "    for text in tweets:\n",
    "        url = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)\n",
    "        url_list.append(' '.join(url))\n",
    "        url_count.append(len(url))\n",
    "    return (url_list, url_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.text = data.text.apply(expandContractions)\n",
    "tweet_token, n_token = tweet_func(data)\n",
    "hashtag, n_hashtag = hashtag_func(data)\n",
    "url, n_url = url_func(data)\n",
    "mention, n_mention = mention_func(data)\n",
    "print('Functions applied!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing\n",
    "Steps:\n",
    "1. Text vectorizing with CountVectorizer\n",
    "2. Concatenate text vectors\n",
    "3. Encode target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Vectorizing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Hashtag Vectorizer\n",
    "cv_hashtag = CountVectorizer(token_pattern='#\\w+')\n",
    "hashtag_vector_train = cv_hashtag.fit_transform(hashtag[0:len(df)])\n",
    "hashtag_vector_test = cv_hashtag.transform(hashtag[len(df):])\n",
    "print('hashtag vector:')\n",
    "print(type(hashtag_vector_train))\n",
    "print(hashtag_vector_train.shape, '\\n')\n",
    "\n",
    "# Mention Vectorizer\n",
    "cv_mention = CountVectorizer(token_pattern='@\\w+')\n",
    "mention_vector_train = cv_mention.fit_transform(mention[0:len(df)])\n",
    "mention_vector_test = cv_mention.transform(mention[len(df):])\n",
    "print('mention vector:')\n",
    "print(type(mention_vector_train))\n",
    "print(mention_vector_train.shape, '\\n')\n",
    "\n",
    "# Url Vectorizer\n",
    "cv_url = CountVectorizer(token_pattern='http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "url_vector_train = cv_url.fit_transform(url[0:len(df)])\n",
    "url_vector_test = cv_url.transform(url[len(df):])\n",
    "print('url vector:')\n",
    "print(type(url_vector_train))\n",
    "print(url_vector_train.shape, '\\n')\n",
    "\n",
    "# Tweet Vectorizer\n",
    "cv_tweet = CountVectorizer()\n",
    "tweet_vector_train = cv_tweet.fit_transform(tweet_token[0:len(df)])\n",
    "tweet_vector_test = cv_tweet.transform(tweet_token[len(df):])\n",
    "print('tweet vector:')\n",
    "print(type(tweet_vector_train))\n",
    "print(tweet_vector_train.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate text vectors\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "X = hstack([hashtag_vector_train, mention_vector_train, url_vector_train, tweet_vector_train])\n",
    "X = X.todense()\n",
    "print('Training features:')\n",
    "print(type(X))\n",
    "print(X.shape)\n",
    "\n",
    "X_pred = hstack([hashtag_vector_test, mention_vector_test, url_vector_test, tweet_vector_test])\n",
    "X_pred = X_pred.todense()\n",
    "print('Prediction features:')\n",
    "print(type(X_pred))\n",
    "print(X_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "y = to_categorical(df.target)\n",
    "print(type(y))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Model\n",
    "1. n Hidden Layer = 2\n",
    "2. n Nodes = 100\n",
    "3. activation function = 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (X.shape[1],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "def new_model(input_shape=input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation='relu', input_shape=input_shape))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See model performance with 25% validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = new_model(input_shape)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X, y, validation_split=0.25, epochs = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model to the whole training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = new_model(input_shape)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict class probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_pred)\n",
    "print('First 5 predictions:')\n",
    "print(predictions[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for proba in predictions:\n",
    "    if proba[0] > 0.5:\n",
    "        results.append(int(0))\n",
    "    else:\n",
    "        results.append(int(1))\n",
    "results = pd.Series(results)\n",
    "print('Predicted class ratio:')\n",
    "print(results.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create submission file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "print(submission.head())\n",
    "submission.target = results\n",
    "submission.set_index('id', inplace=True)\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv')\n",
    "print('Submission saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTES:**\n",
    "1. I have tried to optimize the keras model by reducing/increasing layers & nodes and so far this is the best I got.\n",
    "2. I have only used ngram = 1 for the vectorizing.\n",
    "\n",
    "Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
